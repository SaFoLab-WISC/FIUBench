# VFUBench: Benchmarking Vision Language Model Unlearning via Fictious Datasets

we introduce **VFUBench**, a new benchmark designed to robustly evaluate VLM unlearning, especially under the **Right to be Forgotten** setting. This benchmark is built on our newly developed Fictitious Entity Visual Question Answering (VQA) dataset, which includes 400 virtual entities, each represented through 20 VQAs focusing on image-related attributes and background knowledge. Based on this dataset, we define VLM unlearning as the process of forgetting the image-text alignment in VLMs and design two VLM unlearning tasks: overall and fine-grained VLM unlearning. Upon applying the unlearning algorithm to our dataset and tasks, our benchmark offers a robust evaluation under the **Right to be Forgotten** scenario, as we can precisely control the source of information and the exposure levels of the dataset so that it can simulate a dataset with rare information to be unlearned. By evaluating four baseline VLM unlearning algorithms in our benchmark, we observe that though all strategies effectively facilitate the forgetting of image-text alignment for **overall VLM unlearning**, most of them fail to realize **fine-grained VLM unlearning**. Besides, all baselines struggle to preserve the model utility of VLMs. These highlight a promising area for further research in developing better VLM unlearning strategies. 
